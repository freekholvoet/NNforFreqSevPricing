---
title: "Neural networks for insurance pricing with frequency and severity data"
author: "Freek Holvoet, Katrien Antonio and Roel Henckaerts"
output:
  html_notebook:
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_folding: none
    theme: cosmo
    number_sections: true
  html_document:
    toc: yes
    df_print: paged
bibliography: biblio.bib 
---

```{css, echo = FALSE}
pre code, pre, code {
white-space: pre !important;
overflow-x: scroll !important;
word-break: keep-all !important;
word-wrap: initial !important;
}
```

<style>
body { text-align: justify}
</style>

This R Markdown Notebook accompanies the paper “Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing steps to technical tariff”. As an illustration, we will go through the setup of some of the models and techniques from the paper. 

Because this notebook is to illustrate the techniques from the paper, we have omitted the tuning of each model here. We will instead work with the already optimized tuning parameters from the paper. We will also focus on one data set, only frequency modelling, and will show the construction of one model. This should be sufficient to reconstruct all models from the paper, both for frequency and severity. 

```{r setup, include = FALSE}
library(tidyverse)
ggplot2::theme_set(theme_bw())
ggplot2::theme_update(text = element_text(size = 20))
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Input data

For this notebook, we will be using the Belgian MTPL data set. We start by reading in the data. 

```{r data_read, warning=FALSE}
data_BE <- readRDS('data_BE.rds')

# A look at the data
library(tidyverse)
head(data_BE %>% arrange(id))
```

The following variables are used as predictor features. We define a vector of the categorical features separately. Note that we use the latitude-longitude of the municipality of residence as predictor variables, instead of the postal code. This insures that the model keeps the spatial relationship between different municipalities.

```{r data_vars, results='hide'}
features <- c('coverage', 'fuel', 'sex', 'use', 'fleet', 
              'ageph', 'power', 'agec', 'bm',
              'long', 'lat')
categorical_features <- c("coverage", "fuel", "sex", "use", "fleet" )
```

We split the data up into a training set and a test set. 

```{r train_test, results='hide'}
data_train <- data_BE %>% filter(fold_nr != 1)
data_test <- data_BE %>% filter(fold_nr == 1)
```

# Benchmark GBM

As example in this notebook, we will be using the GBM benchmark model, as constructed by @Henckaerts2021. We read in the already tuned and fitted model here.

```{r bench_gbm, warning=FALSE, results='hide'}
gbm_fits <- readRDS("./Data/mfits_gbm.rds")
gbm_fit_frequency <- gbm_fits[[1]] # We select the frequency Poisson GBM for test set one

# Functions for GBM predicting
library(gbm)
predict_model <- function(object, newdata) UseMethod('predict_model')
predict_model.gbm <- function(object, newdata) {
  predict(object, newdata, n.trees = object$n.trees, type = 'response')
}
```

We take a look at the two tuning parameters of the gradient boosting.

```{r gbm_tuning, warning=FALSE}
# Number of trees
print(gbm_fit_frequency$n.trees)

# Tree depth
print(gbm_fit_frequency$interaction.depth)
```

# Preprocessing

## Continuous input data

The continuous input variables will be normalize by scaling around zero, calculated as:
\[\mathbf{\tilde{x}}=\frac{\mathbf{x}-\mu_{\mathbf{x}}}{\sigma_{\mathbf{x}}},\]

We need to make sure to apply the scaling from the train data set to the test data set. This makes sure all data is scaled in the same way, but we still have not used information from the test set.

```{r scaling, results='hide'}
# Variables to be scaled
scale_vars <- c("ageph","bm","agec","power","long","lat")

# Scaling the training data
data_train_scaled <- data_train %>% 
  mutate_at(scale_vars,scale)

# Determine the mean and st.dev of each scaling variable
scale_mean <- data_train %>% summarise_at(scale_vars,mean)
scale_st.dev <- data_train %>% summarise_at(scale_vars, sd)

# Manually scale the test set based on the mean and st.dev of the training data
data_test_scaled <- data_test %>% 
  select(all_of(scale_vars)) %>% 
  as.matrix() %>%
  scale(.,center = scale_mean, scale = scale_st.dev) %>% #applying the scaling
  as_tibble() %>%
  bind_cols(data_test %>% select(!all_of(scale_vars))) # binding back together with the categorical variables
```

We can take a look at the data, and see that our continuous variables are now scaled.

```{r scaling_head, echo = FALSE}
head(data_train_scaled %>% arrange(id))
```

## Categorical input data

The categorical variables will be embedded using the autoencoder technique from Delong and Kozak, 2021. First, we create a one-hot encoded representation of each categorical variable.

```{r onehot, warning=FALSE, results='hide'}
library(mltools)
library(data.table)

#Create one-hot encoding matrix for each categorical variable
catdata_onehot <- lapply(categorical_features, function(var){
  data_train %>% 
    pull(var) %>% 
    as.data.table() %>% 
    one_hot(cols=".") %>% 
    data.matrix()
})

# Bind all one-hot encoding matrices together
catdata_concatenate <- do.call(cbind,catdata_onehot)
```

This gives us a matrix representing the onehot encoding of all categorical variables. We can now set up the structure of our autoencoder. As dimension of our encoded representation, we use five. This was determined by tuning, which is not shown in this notebook.

```{r autoencoder_structure, warning=FALSE, results='hide'}
library(keras)

# Input and encoder size
input_size = dim(catdata_concatenate)[2]
latent_size = 5 

# Encoder
enc_input = layer_input(shape = input_size, name = "input_layer")

enc_output = enc_input %>% 
  layer_dense(units=latent_size, name = "encoding_layer") # Here we define the size of the encoding

encoder = keras_model(enc_input, enc_output)

# Decoder
dec_input = layer_input(shape = latent_size)

decoder_list <- lapply(catdata_onehot, function(var){
  dec_input %>% layer_dense(units = dim(var)[2], activation = 'softmax')
}) # we apply a softmax activation for each input variable

decoder = keras_model(dec_input, decoder_list)

# Autoencoder
aen_input = layer_input(shape = input_size)

aen_output = aen_input %>% 
  encoder() %>% 
  decoder() # The autoencoder consists of both encoder and decoder

aen = keras_model(aen_input, aen_output) 
```

We can look at the structure of our autoencoder. We have 11 inputs, which is the sum of the number of levels for each categorical variable. The encoder represents the input into five encoded nodes. The output consists of five pieces, one for each categorical variables. These are shows separately, because we apply a softmax function for each group of output nodes representing an input variable.

```{r autoencoder_print, warning=FALSE}
print(aen)
```

We can now fit the autoencoder on our one-hot encoded categorical variables. This takes approximately one minute. The Nadam optimizer is used and the binary cross-entropy as a loss function. Batch size is 1000 and we run for maximum 500 epochs.

```{r autoencoder_fit, results='hide'}
# Compile and fit the autoencoder
aen %>% compile(optimizer = 'nadam',
                loss = 'binary_crossentropy',
                loss_weights = rep(1,length(catdata_onehot)))

history <- aen %>% 
  fit(catdata_concatenate,
      catdata_onehot, 
      epochs = 500, 
      batch = 1000, 
      verbose = FALSE
  ) 
```

Printing the fitted model shows us the accuracy. We can see the accuracy in binary cross-entropy for the autoencoder in total, and for each of the variables separately. 

```{r autoencoder_results, warning=FALSE}
# Accuracy results
print(history)
```

Being content with performance of our autoencoder, we have one last step to do. Because the autoencoder takes our categorical inputs and encodes them into five continuous nodes, we need to apply our normalization step for continuous input variables. 

We start by extracting the matrix of weights from the encoder.

```{r autoencoder_weights, warning=FALSE}
AE_weights <- get_weights(encoder)

print(AE_weights)
```

We can see we have a weight matrix of dimension 5 $\times$ 11 and a bias vector of dimension 5. Note that the weight matrix is shown transposed here, which is the way Keras handles the weigth matrices.

To scale the matrices, we first take the one-hot representation of all categorical variables. We calculate the encoded vector for each input data point. From this, we can calculate the mean and standard deviation from the encoded vectors. 

```{r autoencoder_scalingparam, warning=FALSE}
# One-hot encode each categorical variable
catdata_onehot <- lapply(categorical_features, function(var){
  data_train %>% 
    pull(var) %>% 
    as.data.table() %>% 
    one_hot(cols=".") %>% 
    data.matrix()
}) %>% do.call(cbind,.)

# Calculate the vectors in the latent space
latent_space <- sapply(1:nrow(catdata_onehot), function(datapoint){
  (t(as.matrix(AE_weights[[1]])) %*% as.matrix(catdata_onehot[datapoint,]) + as.vector(AE_weights[[2]]))
}) %>% t

# Calculate the mean and standard deviation of each encoded node
AE_scaling_param <- sapply(1:ncol(latent_space),function(encoded_node){
  return(c(mu = mean(latent_space[,encoded_node]), sigma = sd(latent_space[,encoded_node])))
}) %>% t

print(AE_scaling_param)
```
We can see the values in the encoded nodes are not centered around zero. We use the mean and standard deviation of each encoded node to scale the weight matrix and bias vector.

```{r autoencoder_scalingapplied, warning=FALSE}
# Calculate scaled weights and biases
scaled_weights <- t(as.matrix(AE_weights[[1]]))/as.vector(AE_scaling_param[,2] %>% as.matrix)
scaled_bias <- (t(as.matrix(AE_weights[[2]])) - as.vector(AE_scaling_param[,1] %>% as.matrix))/as.vector(AE_scaling_param[,2] %>% as.matrix)

# Combine in list
AE_weights_scaled <- list(scaled_weights %>% t,
                          scaled_bias %>% array)

print(AE_weights_scaled)
```

We can check wether the scaling works as intented, by looking at the mean and standard deviation of the encoded nodes if we use the scaled weights matrix and bias vector.

```{r autoencoder_checkscaling, warning=FALSE}
# Calculate the vectors in the latent space
latent_space_scaled <- sapply(1:nrow(catdata_onehot), function(datapoint){
  (t(as.matrix(AE_weights_scaled[[1]])) %*% as.matrix(catdata_onehot[datapoint,]) + as.vector(AE_weights_scaled[[2]]))
}) %>% t

# Calculate the mean and standard deviation of each encoded node
AE_scaled_info <- sapply(1:ncol(latent_space_scaled),function(encoded_node){
  return(c(mu = mean(latent_space_scaled[,encoded_node]), sigma = sd(latent_space_scaled[,encoded_node])))
}) %>% t

print(AE_scaled_info)
```
We can see we normalized the encoded nodes around zero, with standard deviation one. Now that both the continuous and categorical variables are preprocessed, we can start with the construction of our neural networks.

# Combined actuarial neural network

As example in this notebook, we will construct the combined actuarial neural network from @Schelldorfer2019, but with flexible output layer. As initial model input, we use the GBM model as explained above. 

## Initial model predictions

We begin by calculating the GBM predictions.

```{r gbm_predictions, results='hide'}
gbm_prediction <- bind_cols(newdata = data_BE, 
                            prediction = gbm_fit_frequency %>% predict_model(newdata = data_BE) * data_BE$expo)
```

Adding the GBM predictions to both train and test data set.

```{r add_gbm, results='hide'}
data_train_GBM <- data_train_scaled %>% left_join(., gbm_prediction %>% select(c(id,prediction)), by = 'id') %>% 
  select(!c(id, expo, average, fold_nr)) # deselect not used variables

data_test_GBM <- data_test_scaled %>% left_join(., gbm_prediction %>% select(c(id,prediction)), by = 'id') %>% 
  select(!c(id, expo, average, fold_nr)) # deselect not used variables
```

## Constructing of the CANN model

The tuning of the networks is omitted in this notebook. Instead, we read in the optimal tuning parameters. 

```{r opt_tuning, warning=FALSE}
# Read in tuning results and select CANN model with GBM input and flexible output layer
load('optimal_tuning_param_BE')
tuning_parameters <- BE_NC_CANN_GBM_flex[[1]]

# A look at the tuning parameters
print(tuning_parameters)
print(tuning_parameters$hiddennodes)
```

So we have a neural network with two hidden layers, each consisting of 17 nodes and sigmoid activation function. We use the Adam optimizer, with a initialized learning rate of 0.001 (not shown in the table). A batch size of 14 197 and a dropout rate of 0.0156 is used. With this info, we can construct our neural network.

```{r cann_construct, warning=FALSE}
# Initialize the model
model <- keras_model_sequential()

# Input layer for the continuous variables
cont_layer <- layer_input(shape = 6, dtype='float32', name = "continuous_input_layer")

# Input layer for the categorical variables, size is the size of the one-hot representation of the cat variables
cat_input <- layer_input(shape = 11, name = "cat_input_layer")
# We add the trained encoder weights to the categorical inputs
AE_layer <- cat_input %>% 
    layer_dense(units = 5, weights = AE_weights_scaled, name = "cat_encoded_layer")

# Add both inputs together
net <- c(cont_layer,AE_layer) %>% layer_concatenate(name = "combined_input_layer")

# Add both hidden layers
net <- net %>% 
  layer_dense(units = 17, 
              activation = 'sigmoid',
              name = "hidden_layer_1"
  ) %>% 
  layer_dropout(0.0156, name = "dropout_layer_1") %>% 
  layer_dense(units = 17, 
              activation = 'sigmoid',
              name = "hidden_layer_2"
  ) %>% 
  layer_dropout(0.0156, name = "dropout_layer_2")

# Add output node of the neural network adjustment
net <- net %>% layer_dense(units = 1, activation = "linear", name = "adjustment_netwerk_prediction")

# Input layer for the initial model predictions
cann_input <- layer_input(shape=c(1),dtype='float32', name = "initial_model_input_layer")

# Add the initial model input and the neural network adjustment together and add the final output layer
response = list(net, cann_input) %>% 
    layer_concatenate(name = "cann_combination_layer") %>% 
    layer_dense(units = 1L, 
                input_shape = 2L,
                # this gives the CANN a flexible output layer
                trainable = TRUE, 
                weights = list(matrix(c(1,1), nrow=2), array(c(0))),
                activation = 'exponential',
                name = "cann_output_layer")

# Make the total model
model <- keras_model(inputs = c(cont_layer, cat_input, cann_input), outputs = response, name = "Cann_model_with_GBM_input_flexible_output")

# Take a look at the model structure
print(model)
```

We can visualize the model output using the DeepVIZ package. We can see how the categorical variables are embedding and then joined with the continuous variables. They go through the hidden layers with dropout layers. The adjustment network prediction is made with a linear activation and then joined with the initial model input. 

```{r deepviz, echo='FALSE', results='hide'}
#devtools::install_github("andrie/deepviz")
library(deepviz)
library(magrittr)
```

```{r modelviz, warning=FALSE}
# devtools::install_github("andrie/deepviz")
model %>% plot_model()
```


## Model fitting and performance

We are now ready to train the model structure we have build. The model takes three inputs; a matrix with the continuous variables, a matrix with the one-hot encoded categorical variables and a vector with the predictions from our GBM. Because we use an exponential activation on the output node, we apply a log transform on the GBM predictions. 

```{r cann_inputdata, results='hide'}
# Prepare training data
data_train_CONT <- data_train_GBM %>% select(c("ageph","bm","agec","power","long","lat")) %>% data.matrix()
data_train_CAT <- lapply(categorical_features, function(var){
  data_train_GBM %>% 
    pull(var) %>% 
    as.data.table() %>% 
    one_hot(cols=".") %>% 
    data.matrix()
}) %>% do.call(cbind,.)
data_train_GBMpred <- data_train_GBM %>% select(prediction) %>% log %>% data.matrix()
# We apply a log transform on the initial model input, because we have an exponential activation in the output layer

data_train_matrices <- list(data_train_CONT, data_train_CAT, data_train_GBMpred)

# Prepare test data
data_test_CONT <- data_test_GBM %>% select(c("ageph","bm","agec","power","long","lat")) %>% data.matrix()
data_test_CAT <- lapply(categorical_features, function(var){
  data_test_GBM %>% 
    pull(var) %>% 
    as.data.table() %>% 
    one_hot(cols=".") %>% 
    data.matrix()
}) %>% do.call(cbind,.)
data_test_GBMpred <- data_test_GBM %>% select(prediction) %>% log %>% data.matrix()
# We apply a log transform on the initial model input, because we have an exponential activation in the output layer

data_test_matrices <- list(data_test_CONT, data_test_CAT, data_test_GBMpred)
```

We can now fit the model, using early stopping with a 20% randomly chosen validation set. We use the custom defined Poisson deviance as loss function.

```{r cann_fitting, results='hide'}
# Our chosen loss function is the Poisson deviance
poisson_metric <- function(y_true, y_pred){
  K <- backend()
  loss <- 2*K$mean(y_pred - y_true - y_true * (K$log(y_pred) - K$log(y_true+ 0.00000001)))
  loss
}
metric_poisson_metric <- custom_metric("poisson_metric", function(y_true, y_pred) {
  poisson_metric(y_true, y_pred)
})

# Compile the model with correct loss function
model %>% compile(
  loss = "poisson",
  optimizer = 'adam',
  metrics = metric_poisson_metric
)

# Fit the model on the train data
history <- model %>%
  fit(x = data_train_matrices, 
      y = data_train_GBM %>% pull(nclaims) %>% data.matrix(), #The response we want to train toward
      epochs = 500,
      batch_size = 14197,
      callbacks = callback_early_stopping(monitor = "val_poisson_metric", patience = 20),
      validation_split = 0.2,
      verbose=0
  )
```

With the trained model, we can calculate the out-of-sample performance on the test set. 

```{r oos, warning=FALSE, message=FALSE}
# Poisson deviance function to calculate out-of-sample performance
dev_poiss <- function(ytrue, yhat) {
  -2 * mean(dpois(ytrue, yhat, log = TRUE) - dpois(ytrue, ytrue, log = TRUE), na.rm = TRUE)
}

# Predict on the test set with the model
oos_predictions <- model %>% predict(data_test_matrices, verbose=0)

# Calculate the Poisson loss
dev_poiss(data_test_GBM %>% pull(nclaims) %>% data.matrix(), oos_predictions)
```

# Interpretation tools 

In the paper, we look at two different interpretation tools; variable importance and partial dependency. 

## Custom prediction function

Both interpretation techniques rely on mutating the input data and then looking at the predictions on the mutated data. Our neural network takes a list of matrices as inputs; the continuous inputs matrix, one-hot encoded matrix and the initial models predictions vector. On top of that, the continuous input variables need to be scaled, and with the mutated input data we need to make new initial model predictions. Therefore, we will write a custom prediction function to make things easier on us.

We start by making a table of the scaling parameters for each continuous variable.

```{r pred_scaleinfo, results='hide'}
NC_scaleinfo_BE <- expand.grid(Variable = c("ageph", "bm", "agec", "power", "long", "lat")) %>% 
  as_tibble %>% 
  rowwise %>%
  mutate(u = (data_train %>% pull(Variable) %>% mean),
         sd = (data_train %>% pull(Variable) %>% sd))

# Function to apply scaling to each variable with the parameters
scale_withPar <- function(data, scale_vars, scale_pars){
  bind_cols(
    data %>% 
      select(scale_vars) %>% 
      scale(., 
            center = (scale_pars  %>% pull(u)), 
            scale = (scale_pars %>% pull(sd))) %>%
      as_tibble,
    data %>% 
      select(!scale_vars)
  ) %>% select(colnames(data))
}
```

One last manipulation we need to add in the custom prediction function, is the swapping from postal code to latitude-longitude. The original input data included the postal code of the policyholder. We replaced this to the latitude-longitude coordinates, which captures the spatial relationship between postal codes much better. But if we want to make changes to the input data, we want to make the changes on the level of the postal code, and then translate that back into latitude-longitude coordinates.

```{r pred_spatialinfo, results='hide', warning=FALSE}
# Complete list of all postal codes in Belgium with lat long of center
library(rgdal)
belgium_shape <- readOGR('./shape file Belgie postcodes/npc96_region_Project1.shp') %>% spTransform(CRS('+proj=longlat +datum=WGS84'))
latlong_per_postalcode <- bind_cols(belgium_shape@data %>% as_tibble %>% select(postcode = POSTCODE), 
                         sp::coordinates(belgium_shape) %>% as_tibble %>% rename(lat = V2, long = V1) )
```

We add back the postal code to the input data.

```{r data_with_PC, results='hide'}
# Add the Postal code back to Belgian data; we want effect of Postal code, not of lat-long separately
data_readin <- readRDS("./Data/Data.rds") # Original data with postalcodes
data_train_PC <- data_train %>% left_join(data_readin %>% select(id,postcode), by = 'id') %>% select(!c(lat, long))
data_test_PC <- data_test %>% left_join(data_readin %>% select(id,postcode), by = 'id') %>% select(!c(lat, long))
```

We are now ready to write our custom prediction function.

```{r pred_function, results='hide'}
custom_prediction <- function(object, newdata){
  
  # One-hot encode all categorical variables
  train_cat_data <- lapply(object$cat_vars,function(var_FH){
    newdata %>% 
      dplyr::pull(var_FH) %>% 
      data.table::as.data.table() %>% 
      mltools::one_hot(cols=".") %>% 
      data.matrix()
  })
  
  # Bind the one-hot variables into a matrix
  train_cat_data_concat <- do.call('cbind',train_cat_data)
  
  # If a latlong conversion is supplied, apply it
  cont_data_LL <- newdata %>% 
    left_join(object$latlong_conversion,by=c("postcode"))
  
  # Scale the continuous variables
  cont_data <- cont_data_LL %>%
    dplyr::select(object$cont_vars) %>%
    scale_withPar(object$cont_vars, object$scale_info)
  
  # Add the GBM predictions, offset based on whether it is frequency or severity model
  new_prediction <- predict_model.gbm(object = object$GBM_model, newdata = cont_data_LL)* newdata$expo
  
  # Bind all data into a list of matrices
  train_mat <- list(cont_data %>% data.matrix(),
                    train_cat_data_concat,
                    new_prediction %>% log %>% data.matrix())
  
  # Make predictions on the data with the supplied CANN model
  return(object$NN_model %>% predict(train_mat, type = "response", verbose = 0) %>% mean)
}
```

We bind our network together with the necessary information for the custom prediction function.

```{r objectdef, warning=FALSE}
model_object <- list(NN_model = model,
                     GBM_model = gbm_fit_frequency,
                     cat_vars = c("coverage", "fuel", "sex", "use", "fleet"),
                     cont_vars = c("ageph", "bm", "agec", "power", "long", "lat"),
                     scale_info = NC_scaleinfo_BE,
                     latlong_conversion = latlong_per_postalcode)
```

## Variable importance

To calculate the importance of each variable relative to the models' predictions, we follow the permutational technique from @Olden2004. The technique consists at looking at the change in average prediction when we permutate a variable. 

We first make predictions on the original data set. 
```{r vip_step1, warning=FALSE}
# We take a sample of the data, to run faster
data_slice <- data_train_PC %>% slice_sample(n=10000)

average_prediction <- custom_prediction(model_object, data_slice) %>% mean

print(average_prediction)
```

So the average predicted number of claims on our data sample is `r round(average_prediction, digits= 4)`. We can now permutate each variable and look at the difference in average prediction.

```{r vip_step2, results='hide'}
# For each variable, permutate and predict
VI <- lapply(c("coverage", "fuel", "sex", "use", "fleet", "ageph", "bm", "agec", "power", "postcode"), function(var){
  
  # Permutate the variable
  permutated_data <- data_slice %>% mutate(!!var := (slice_sample(., n=nrow(.)) %>% pull(var)))
  
  # Make the preduction
  mut_prediction <- custom_prediction(model_object, permutated_data)
  
  # Calculate the difference with the unmutated predictions
  tibble(Variable = var, VI =  sum(abs(average_prediction - mut_prediction)))
  
}) %>% do.call(rbind,.) %>% mutate(scaled_VI = VI / sum(VI))
```

We plot the variable importance for each variable, ordered by importance.

```{r vip_plotsetup, results='hide'}
VI_plot <- VI %>% 
  ggplot(aes(y = reorder(Variable, scaled_VI, mean))) +  
  geom_col(aes(x = scaled_VI), position="dodge", alpha = 0.6) + 
  theme_bw() + 
  guides(color = guide_legend(nrow = 1, byrow = TRUE)) + 
  xlab("Importance") + ylab("Covariates") + 
  theme(legend.position="bottom", legend.direction="horizontal", 
        plot.title = element_text(size=18, margin=margin(0,0,50,0)),
        axis.title=element_text(size=16), plot.title.position = "plot")
```

<center>

```{r vip_plot, fig.align="center", fig.height = 4, fig.width = 4, echo=FALSE}
VI_plot
```
</center>

## Partial dependency

To calculate the partial dependency effect, we use the Maidrr package from @Henckaerts2022. The package uses the technique as described in @Freidmanetal2001. As example, we look at the partial dependency effect of the policyholder age.

```{r pd_effects, warning = FALSE, results='hide'}
library(maidrr)

PD_AgePH <- get_pd(
  mfit = model_object,
  var = 'ageph',
  grid = data.frame(ageph = c(18:95)),
  data = data_slice, 
  fun = custom_prediction,
  ncores = 1
)

PD_BonusMalus <- get_pd(
  mfit = model_object,
  var = 'bm',
  grid = data.frame(bm = c(0:22)),
  data = data_slice, 
  fun = custom_prediction,
  ncores = 1
)
```

We can plot the partial dependency effects. 

```{r pd_plot_setup, fig.height = 4, fig.width = 8}
gridExtra::grid.arrange(
  PD_AgePH %>% 
    ggplot(aes(x = x)) + geom_line(aes(y = y), size = 0.8) + 
    theme_bw() + 
    guides(group = guide_legend(nrow = 2, byrow = TRUE)) + 
    theme(legend.position="bottom", 
          legend.direction="horizontal") + 
    xlab("Policyholder age") + 
    ylab("Partial dependency effect"),
  PD_BonusMalus %>% 
    ggplot(aes(x = x)) + geom_line(aes(y = y), size = 0.8) + 
    theme_bw() + 
    guides(group = guide_legend(nrow = 2, byrow = TRUE)) + 
    theme(legend.position="bottom", 
          legend.direction="horizontal") + 
    xlab("Bonus-malus score") + labs(y=NULL),
ncol = 2)
```

# Global surrogate model

We use the surrogate technique from @Henckaerts2022 to relate the partial dependency effects learned from the neural networks back into a GLM. The Maidrr function autotune is used to tune the dynamic programming. Tuning and fitting the surrogate GLM is quite computationally expensive. We show the code used here, but instead of running that code, we read in the already tuned and fitted results.

```{r eval=FALSE}
# Tune and fit the surrogate GLM
NC_Surrogate_CANN_GBM_flex_BE <- maidrr::autotune(model_object,
                                    data = data_train_PC,
                                    vars = c("coverage", "fuel", "sex", "use", "fleet",
                                             "ageph", "bm", "agec", "power", "postcode"),
                                    target = 'nclaims',
                                    hcut = 0.75,
                                    pred_fun = custom_prediction,
                                    lambdas = as.vector(outer(seq(1, 10, 2), 10^(-6:-2))), # Tuning parameters
                                    max_ngrps = 15,
                                    nfolds = 5,
                                    strat_vars = c('nclaims', 'expo'),
                                    glm_par = alist(family = poisson(link = 'log'),
                                                    offset = log(expo)),
                                    err_fun = maidrr::poi_dev,
                                    out_pds = TRUE,
                                    ncores = 1)
```


We read in the results of the code above.

```{r surr_readin}
# Read in already fitted surrogate model
load('NC_Surrogate_CANN_GBM_flex_BE')
```

We take a look at the results. First item is the number of splits for each variable. We can also see here which cross effects between variables were seen as significant by the surrogate fitting.

```{r surr_look1}
NC_Surrogate_CANN_GBM_flex_BE$slct_feat
```
Second is the fitted GLM with the optimal splits. We can also see here where the splitting points are.

```{r surr_look2}
NC_Surrogate_CANN_GBM_flex_BE$best_surr
```

From the optimal GLM, we can also read in the segmented data. 

```{r surr_look3}
NC_Surrogate_CANN_GBM_flex_BE$best_surr$data %>% select(c(ends_with("_"),'expo', 'nclaims')) %>% head()
```

We want to look at the out-of-sample perforamance of our surrogate GLM. We can do this by using the tuned data segmentation on our test set. 

```{r oos_surr}
# We use the Maidrr segmentation function to apply our segmentation to the test data.
data_test_segm <- segmentation(fx_vars = NC_Surrogate_CANN_GBM_flex_BE$pd_fx[names(NC_Surrogate_CANN_GBM_flex_BE$slct_feat)], 
                               data = data_test_PC , 
                               type = 'ngroups', 
                               values = NC_Surrogate_CANN_GBM_flex_BE$slct_feat)

# Make predictions on the segmented test set with the surrogate GLM
test_predictions <- predict(NC_Surrogate_CANN_GBM_flex_BE$best_surr, 
                  data_test_segm %>% select(c(ends_with("_"),'expo', 'nclaims')), 
                  type = 'response')

# Calculate the out-of-sample Poisson deviance on the test set.
dev_poiss(data_test_segm %>% pull(nclaims), 
          test_predictions)
```
# Conclusion

We hope that with this notebook, we have illustrated how we constructed the used models and techniques from our paper sufficiently. The construction of the autoencoders is shown and how to setup a CANN model with autoencoder embedding and flexible output layer. The tuning is omitted, but anyone understanding cross-validation can extrapolate the code shown here to do the tuning. The interpretation techniques are applied to our neural network, as is the surrogate technique.

In the paper, we do not only look at one CANN model but use a GLM and a GBM as initial model input and look at fixed output layers. But the code in this notebook can easily be adjusted to this. The feedforward neural network, without initial model input, can also be constructed in the same way as shown here. To do this, one only needs to omit the cann_combination_layer in constructing the neural net.

# References

